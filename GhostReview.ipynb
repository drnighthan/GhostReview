{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.通过aesthetics scorer量化模型质量\n",
    "### 1.aesthetics scorer for image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API to use txt2img \n",
    "#API调用SD文生图\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def ImageSET(prompt,negative_prompt='(worst quality, low quality,NSFW:2), EasyNegative ,ng_deepnegative_v1_75t',\n",
    "             cfg_scale=6,width=512,height=768,sampler_name='DPM++ 2M Karras',seed=1,batch_size=4,n_iter=2,steps=30,enable_hr=True,hr_upscaler=\"4x-UltraSharp\"):\n",
    "  ImagePayload = {\n",
    "    \"enable_hr\": enable_hr,\n",
    "    \"denoising_strength\": 0.5,\n",
    "    \"hr_scale\": 2,\n",
    "    \"hr_upscaler\": hr_upscaler,\n",
    "    \"prompt\": prompt,\n",
    "    \"seed\": seed,\n",
    "    \"sampler_name\": sampler_name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_iter\": n_iter,\n",
    "    \"steps\": steps,\n",
    "    \"cfg_scale\": cfg_scale,\n",
    "    \"width\": width,\n",
    "    \"height\": height,\n",
    "    \"negative_prompt\": negative_prompt,\n",
    "    \"save_images\": True,\n",
    "  }\n",
    "  return ImagePayload\n",
    "\n",
    "def OptionsSET(sd_model_checkpoint=\"GhostMix-V2.0-fp16-NoVAE\",sd_vae = \"vae-ft-mse-840000-ema-pruned.safetensors\", outdir_txt2img_samples = \"./Image/ImageRating\",promptindex=''):\n",
    "  if len(str(sd_model_checkpoint))*len(str(promptindex)) !=0:\n",
    "    foldername = str(sd_model_checkpoint)+'-'+str(promptindex)\n",
    "    outdir_txt2img_samples = os.path.join(outdir_txt2img_samples,foldername)\n",
    "  OptionsPayload = {\n",
    "    \"sd_model_checkpoint\": sd_model_checkpoint,\n",
    "    \"sd_vae\": sd_vae,\n",
    "    \"outdir_txt2img_samples\": outdir_txt2img_samples\n",
    "  }\n",
    "  return OptionsPayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API to use txt2img \n",
    "#API调用SD文生图\n",
    "#Get Prompts from PromptsDataFrame\n",
    "#通过PromptsDataFrame提取出对应的参数\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PromptDataFrame= pd.read_csv('./PromptsForReviews/PromptsSummaryFromCivitai.csv')\n",
    "PromptDataFrameNotDone = PromptDataFrame[PromptDataFrame['Skip']!=True]\n",
    "\n",
    "url = \"http://127.0.0.1:7860\"\n",
    "\n",
    "for i in range(len(PromptDataFrameNotDone)):\n",
    "    temp = PromptDataFrameNotDone.iloc[i].dropna()\n",
    "    Prompts = temp['Prompts']\n",
    "    sd_model_checkpoint=temp['sd_model_checkpoint']\n",
    "    optionsjson = OptionsSET(sd_model_checkpoint=sd_model_checkpoint,promptindex=str(temp['index']))\n",
    "    imagejson = ImageSET(Prompts,cfg_scale=6,batch_size=4,n_iter=8)\n",
    "    optionspost = requests.post(url=f'{url}/sdapi/v1/options', json=optionsjson)\n",
    "    imagepost = requests.post(url=f'{url}/sdapi/v1/txt2img', json=imagejson)\n",
    "    PromptDataFrame.loc[(PromptDataFrame['index'] == temp['index'])&(PromptDataFrame['sd_model_checkpoint'] == temp['sd_model_checkpoint']),'Skip'] = True\n",
    "    PromptDataFrame.to_csv('./PromptsForReviews/PromptsSummaryFromCivitai.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aesthetics scorer for 1 image method 1\n",
    "#1张图片的美学分数method 1\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"aesthetics-scorer-main\")\n",
    "import torch\n",
    "from aesthetics_scorer.model import preprocess, load_model\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "MODEL = \"laion/CLIP-ViT-L-14-laion2B-s32B-b82K\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = CLIPModel.from_pretrained(MODEL)\n",
    "vision_model = model.vision_model\n",
    "vision_model.to(DEVICE)\n",
    "del model\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL)\n",
    "\n",
    "rating_model = load_model(\"aesthetics-scorer-main/aesthetics_scorer/models/aesthetics_scorer_rating_openclip_vit_l_14.pth\").to(DEVICE)\n",
    "artifacts_model = load_model(\"aesthetics-scorer-main/aesthetics_scorer/models/aesthetics_scorer_artifacts_openclip_vit_l_14.pth\").to(DEVICE)\n",
    "\n",
    "def predict(img):\n",
    "    inputs = clip_processor(images=img, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vision_output = vision_model(**inputs)\n",
    "    pooled_output = vision_output.pooler_output\n",
    "    embedding = preprocess(pooled_output)\n",
    "    with torch.no_grad():\n",
    "        rating = rating_model(embedding)\n",
    "        artifact = artifacts_model(embedding)\n",
    "    return rating.detach().cpu().item(), artifact.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "#LAION aesthetics scorer for 1 image\n",
    "#LAION 1张图片的美学分数\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, input_size, xcol='emb', ycol='avg_rating'):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.xcol = xcol\n",
    "        self.ycol = ycol\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            #nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            #nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "            x = batch[self.xcol]\n",
    "            y = batch[self.ycol].reshape(-1, 1)\n",
    "            x_hat = self.layers(x)\n",
    "            loss = F.mse_loss(x_hat, y)\n",
    "            return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[self.xcol]\n",
    "        y = batch[self.ycol].reshape(-1, 1)\n",
    "        x_hat = self.layers(x)\n",
    "        loss = F.mse_loss(x_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2 == 0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def PredictionLAION(image):\n",
    "    model = MLP(768)  # CLIP embedding dim is 768 for CLIP ViT L 14\n",
    "    s = torch.load(\"improved-aesthetic-predictor-main/sac+logos+ava1-l14-linearMSE.pth\")   # load the model you trained previously or the model available in this repo\n",
    "    model.load_state_dict(s)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model2, preprocess = clip.load(\"ViT-L/14\", device=device)  #RN50x64   \n",
    "    image = preprocess(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model2.encode_image(image)\n",
    "    im_emb_arr = normalized(image_features.cpu().detach().numpy() )\n",
    "    prediction = model(torch.from_numpy(im_emb_arr).to(device).type(torch.cuda.FloatTensor))\n",
    "    return(float(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ClipScore for 1 image\n",
    "#1张图片的ClipScore\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "    model, preprocess = clip.load('ViT-L/14')\n",
    "    \n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text],truncate=True)\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input)\n",
    "        text_features = model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#save rating for each image\n",
    "#对每一张图片进行评分并保存结果\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "\n",
    "#image path\n",
    "#读取图片路径\n",
    "path='Image/ImageRating'\n",
    "PromptsFolderPath = os.path.join(os.getcwd(),path)\n",
    "PromptsFolder = os.listdir(PromptsFolderPath)\n",
    "#prompt of image\n",
    "#读取图片对应的Prompts\n",
    "PromptDataFrame= pd.read_csv('./PromptsForReviews/PromptsSummaryFromCivitai.csv')\n",
    "PromptsList = list(PromptDataFrame['Prompts'])\n",
    "#skip prompts already done\n",
    "#跳过已经做过的Prompts\n",
    "try:\n",
    "    DataSummaryDone = pd.read_csv('./ImageRatingSummary/ImageRatingSummary.csv')\n",
    "    PromptsNotDone = [i for i in PromptsFolder if i not in list(DataSummaryDone['name'])]\n",
    "except:\n",
    "    DataSummaryDone = pd.DataFrame()\n",
    "    PromptsNotDone = [i for i in PromptsFolder]\n",
    "DataSummary = pd.DataFrame()\n",
    "for i in PromptsNotDone:\n",
    "    FolderPath = os.path.join(PromptsFolderPath,str(i))\n",
    "    ImageInFolder = os.listdir(FolderPath)\n",
    "    DataCollect = pd.DataFrame()\n",
    "    index = int(i.split('-')[-1])\n",
    "    txt = PromptsList[index]\n",
    "    for z in ImageInFolder:\n",
    "        ImagePath = os.path.join(FolderPath,z)\n",
    "        Img = Image.open(ImagePath)\n",
    "        #Clipscore\n",
    "        ImgClipScore = get_clip_score(Img,txt)\n",
    "        #aesthetics scorer\n",
    "        #ImageScore = predict(Img)\n",
    "        #LAION aesthetics scorer\n",
    "        ImageLAIONScore = PredictionLAION(Img)\n",
    "        #temp = list(ImageScore)\n",
    "        temp = list()\n",
    "        temp.append(float(ImgClipScore))\n",
    "        temp.append(ImageLAIONScore)\n",
    "        temp = pd.DataFrame(temp)\n",
    "        DataCollect = pd.concat([DataCollect,temp],axis=1)\n",
    "    DataCollect = DataCollect.T\n",
    "    DataCollect['ImageIndex'] = [i+1 for i in range(len(ImageInFolder))]\n",
    "    #DataCollect.columns = ['Rating','Artifact','ClipScore','LAIONScore','ImageIndex']\n",
    "    DataCollect.columns = ['ClipScore','LAIONScore','ImageIndex']\n",
    "    #保存原数据\n",
    "    DataFramePath = './dataresult/ImageRatingDataFrame'\n",
    "    DataCollect.to_csv(os.path.join(DataFramePath,str(i)+'.csv'))\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataSummary,analysis each prompt\n",
    "#DataSummary,每一个prompt的统计分析\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "#image path\n",
    "#读取图片路径\n",
    "path='./Image/ImageRating'\n",
    "PromptsFolder = os.listdir(path)\n",
    "#read summary data\n",
    "#读取总结数据\n",
    "PromptDataFrame= pd.read_csv('./PromptsForReviews/PromptsSummaryFromCivitai.csv')\n",
    "#skip prompts already done\n",
    "#跳过已经做过的Prompts\n",
    "try:\n",
    "    DataSummary = pd.read_csv('./ImageRatingSummary/ImageRatingSummary.csv')\n",
    "    PromptsNotDone = [i for i in PromptsFolder if i not in list(DataSummary['name'])]\n",
    "except:\n",
    "    DataSummary = pd.DataFrame()\n",
    "    PromptsNotDone = [i for i in PromptsFolder]\n",
    "for i in PromptsNotDone:\n",
    "    DataCollect = pd.read_csv(os.path.join('dataresult/ImageRatingDataFrame',str(i)+'.csv'))\n",
    "    temp = pd.DataFrame(DataCollect['LAIONScore'].describe()).T\n",
    "    temp['skew'] = scipy.stats.skew(DataCollect['LAIONScore'], axis=0, bias=True, nan_policy=\"propagate\") \n",
    "    temp['kurtosis'] = scipy.stats.kurtosis(DataCollect['LAIONScore'], axis=0, fisher=True, bias=True, nan_policy=\"propagate\")\n",
    "    temp.columns = [i+'_LAIONScore' for i in list(temp.columns)]\n",
    "    #temp['RatingScore_mean']=np.mean(DataCollect['Rating'])\n",
    "    #temp['RatingScore_std']=np.std(DataCollect['Rating'])\n",
    "    temp['Clipscore_mean']=np.mean(DataCollect['ClipScore'])\n",
    "    temp['Clipscore_std']=np.std(DataCollect['ClipScore'])\n",
    "    #temp['Artifact_mean']=np.mean(DataCollect['Artifact'])\n",
    "    #temp['Artifact_std']=np.std(DataCollect['Artifact'])\n",
    "    temp['name'] = str(i)\n",
    "    DataSummary = pd.concat([DataSummary,temp],axis=0)\n",
    "DataSummary.to_csv('./ImageRatingSummary/ImageRatingSummary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.通过GramMatix计算风格损失,量化模型画风泛化能力\n",
    "### 2.Using GramMatix to calculate the styleloss of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#通过GramMatrix计算风格损失(512,512)\n",
    "#GramMatrix of Styleloss(512,512)\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "model=models.vgg19(pretrained=True).features\n",
    "device=torch.device( \"cuda\" if (torch.cuda.is_available()) else 'cpu')\n",
    "\n",
    "def image_loader(path):\n",
    "    image=Image.open(path).convert('RGB')\n",
    "    rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    loader=transforms.Compose([transforms.Resize((512,512)), transforms.ToTensor(),transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "    image=loader(image).unsqueeze(0)\n",
    "    return image.to(device,torch.float)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG,self).__init__()\n",
    "        self.req_features= ['0','5','10','19','28'] \n",
    "        self.model=models.vgg19(pretrained=True).features[:29] \n",
    "   \n",
    "    def forward(self,x):\n",
    "        features=[]\n",
    "        for layer_num,layer in enumerate(self.model):\n",
    "            x=layer(x)\n",
    "            if (str(layer_num) in self.req_features):\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "model=VGG().to(device).eval()\n",
    "\n",
    "\n",
    "def calc_style_loss(target,style):\n",
    "    batch_size,channel,height,width=target.shape\n",
    "    G=torch.mm(target.view(channel,height*width),target.view(channel,height*width).t())\n",
    "    A=torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n",
    "    style_l=torch.mean((G-A)**2)\n",
    "    return style_l\n",
    "\n",
    "\n",
    "def StyleLossImage(targetimagepath,styleimagepath):\n",
    "    targetimage=image_loader(targetimagepath)\n",
    "    styleimage=image_loader(styleimagepath)\n",
    "    target_feature=model(targetimage)\n",
    "    style_feature=model(styleimage)\n",
    "    styleloss= 0\n",
    "    for tar,style in zip(target_feature,style_feature):\n",
    "        styleloss+=calc_style_loss(tar,style)\n",
    "    return(float(styleloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API to use txt2img(512,512)\n",
    "#API调用SD文生图(512,512)\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def ImageSET(prompt,negative_prompt='(worst quality, low quality,NSFW:2), EasyNegative ,ng_deepnegative_v1_75t',\n",
    "             cfg_scale=6,width=512,height=768,sampler_name='DPM++ 2M Karras',seed=1,batch_size=4,n_iter=2,steps=30,enable_hr=True,hr_upscaler=\"4x-UltraSharp\"):\n",
    "  ImagePayload = {\n",
    "    \"enable_hr\": enable_hr,\n",
    "    \"denoising_strength\": 0.5,\n",
    "    \"hr_scale\": 2,\n",
    "    \"hr_upscaler\": hr_upscaler,\n",
    "    \"prompt\": prompt,\n",
    "    \"seed\": seed,\n",
    "    \"sampler_name\": sampler_name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_iter\": n_iter,\n",
    "    \"steps\": steps,\n",
    "    \"cfg_scale\": cfg_scale,\n",
    "    \"width\": width,\n",
    "    \"height\": height,\n",
    "    \"negative_prompt\": negative_prompt,\n",
    "    \"save_images\": True,\n",
    "  }\n",
    "  return ImagePayload\n",
    "\n",
    "def OptionsSET(sd_model_checkpoint=\"GhostMix-V2.0-fp16-NoVAE\",sd_vae = \"vae-ft-mse-840000-ema-pruned.safetensors\", outdir_txt2img_samples = \"./Image/ImageRating\",promptindex=''):\n",
    "  if len(str(sd_model_checkpoint))*len(str(promptindex)) !=0:\n",
    "    foldername = str(sd_model_checkpoint)+'-'+str(promptindex)\n",
    "    outdir_txt2img_samples = os.path.join(outdir_txt2img_samples,foldername)\n",
    "  OptionsPayload = {\n",
    "    \"sd_model_checkpoint\": sd_model_checkpoint,\n",
    "    \"sd_vae\": sd_vae,\n",
    "    \"outdir_txt2img_samples\": outdir_txt2img_samples\n",
    "  }\n",
    "  return OptionsPayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API to use txt2img \n",
    "#API调用SD文生图\n",
    "#Get Prompts from PromptsDataFrame\n",
    "#通过PromptsDataFrame提取出对应的参数\n",
    "\n",
    "StylePromptDataFrame= pd.read_csv('./PromptsForReviews/StylePrompts.csv')\n",
    "StylePromptDataFrameNotDone = StylePromptDataFrame[StylePromptDataFrame['Skip']!=True]\n",
    "\n",
    "url = \"http://127.0.0.1:7860\"\n",
    "\n",
    "for i in range(len(StylePromptDataFrameNotDone)):\n",
    "    temp = StylePromptDataFrameNotDone.iloc[i].dropna()\n",
    "    Prompts = temp['Prompts']\n",
    "    sd_model_checkpoint=temp['sd_model_checkpoint']\n",
    "    optionsjson = OptionsSET(sd_model_checkpoint=sd_model_checkpoint,outdir_txt2img_samples = \"./GhostReview/Image/ImageRatingStyle\",promptindex='Style-'+str(temp['index']))\n",
    "    imagejson = ImageSET(Prompts,cfg_scale=6,height=512,batch_size=4,n_iter=8)\n",
    "    optionspost = requests.post(url=f'{url}/sdapi/v1/options', json=optionsjson)\n",
    "    imagepost = requests.post(url=f'{url}/sdapi/v1/txt2img', json=imagejson)\n",
    "    StylePromptDataFrame.loc[(StylePromptDataFrame['index'] == temp['index'])&(StylePromptDataFrame['sd_model_checkpoint'] == temp['sd_model_checkpoint']),'Skip'] = True\n",
    "    StylePromptDataFrame.to_csv('./PromptsForReviews/StylePrompts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过GramMatrix计算生成图片与目标图片的风格损失\n",
    "#calculate styleloss of target image and gen images\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "StylePATH = os.listdir('./Image/ImageRatingStyle')\n",
    "\n",
    "path='./Image/ImageRatingStyle'\n",
    "StyleFolder = os.listdir(path)\n",
    "\n",
    "StyleDataFrame= pd.read_csv('./PromptsForReviews/StylePrompts.csv')\n",
    "try:\n",
    "   DataSummary = pd.read_csv('./ImageRatingSummary/ImageRatingStyle.csv')\n",
    "   StyleNotDone = [i for i in StyleFolder if i not in list(DataSummary['name'])]\n",
    "except:\n",
    "   DataSummary = pd.DataFrame()\n",
    "   StyleNotDone = [i for i in StyleFolder]\n",
    "\n",
    "for i in StyleNotDone:\n",
    "    ImageFolderPATH = os.path.join('./Image/ImageRatingStyle',i)\n",
    "    temp = list()\n",
    "    Imagename = list()\n",
    "    Targetname = list()\n",
    "    Imagelist = os.listdir(ImageFolderPATH)\n",
    "    TargetFolderPATH = os.path.join('./Image/TargetForReviews/StyleReview',i.split('-')[-1])\n",
    "    TargetImagelist = os.listdir(TargetFolderPATH)\n",
    "    for z in Imagelist:\n",
    "        ImagePath = os.path.join(ImageFolderPATH,z)\n",
    "        for t in TargetImagelist:\n",
    "            TargetImagePATH = os.path.join(TargetFolderPATH,t)\n",
    "            Imagename.append(z)\n",
    "            Targetname.append(t)\n",
    "            temp.append(StyleLossImage(ImagePath,TargetImagePATH))\n",
    "    temp2 = pd.DataFrame({'StyleLoss':temp,'Image':Imagename,'TargetImage':Targetname})\n",
    "    temp2.to_csv('./dataresult/ImageRatingDataFrameStyle/'+i+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataSummary,summary for each prompts\n",
    "#DataSummary,每一个prompt的统计分析\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "\n",
    "path='Image/ImageRatingStyle'\n",
    "StyleFolder = os.listdir(\"./\"+path)\n",
    "DataSummary = pd.DataFrame()\n",
    "for i in StyleFolder:\n",
    "    DataCollect = pd.read_csv(os.path.join('./dataresult/ImageRatingDataFrameStyle',str(i)+'.csv'))\n",
    "    DataCollect = DataCollect.groupby('Image')\n",
    "    temp = pd.DataFrame(DataCollect['StyleLoss'].mean())\n",
    "    temp['name'] = str(i)\n",
    "    temp['index'] = str(i).split('-')[-1]\n",
    "    DataSummary = pd.concat([DataSummary,temp],axis=0)\n",
    "DataSummary.to_csv('./ImageRatingSummary/ImageRatingStyle.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.通过aesthetics scorer量化模型对LoRA兼容性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API to use txt2img \n",
    "#API调用SD文生图\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "def ImageSET(prompt,negative_prompt='(worst quality, low quality,NSFW:2), EasyNegative ,ng_deepnegative_v1_75t',\n",
    "             cfg_scale=6,width=512,height=768,sampler_name='DPM++ 2M Karras',seed=1,batch_size=4,n_iter=2,steps=30,enable_hr=True,hr_upscaler=\"4x-UltraSharp\"):\n",
    "  ImagePayload = {\n",
    "    \"enable_hr\": enable_hr,\n",
    "    \"denoising_strength\": 0.5,\n",
    "    \"hr_scale\": 2,\n",
    "    \"hr_upscaler\": hr_upscaler,\n",
    "    \"prompt\": prompt,\n",
    "    \"seed\": seed,\n",
    "    \"sampler_name\": sampler_name,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"n_iter\": n_iter,\n",
    "    \"steps\": steps,\n",
    "    \"cfg_scale\": cfg_scale,\n",
    "    \"width\": width,\n",
    "    \"height\": height,\n",
    "    \"negative_prompt\": negative_prompt,\n",
    "    \"save_images\": True,\n",
    "  }\n",
    "  return ImagePayload\n",
    "\n",
    "def OptionsSET(sd_model_checkpoint=\"GhostMix-V2.0-fp16-NoVAE\",sd_vae = \"vae-ft-mse-840000-ema-pruned.safetensors\", outdir_txt2img_samples = \"./Image/ImageRating\",promptindex=''):\n",
    "  if len(str(sd_model_checkpoint))*len(str(promptindex)) !=0:\n",
    "    foldername = str(sd_model_checkpoint)+'-'+str(promptindex)\n",
    "    outdir_txt2img_samples = os.path.join(outdir_txt2img_samples,foldername)\n",
    "  OptionsPayload = {\n",
    "    \"sd_model_checkpoint\": sd_model_checkpoint,\n",
    "    \"sd_vae\": sd_vae,\n",
    "    \"outdir_txt2img_samples\": outdir_txt2img_samples\n",
    "  }\n",
    "  return OptionsPayload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoRA - API to use txt2img \n",
    "#LoRA - API调用SD文生图\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "#Get Prompts from PromptsDataFrame\n",
    "#通过PromptsDataFrame提取出对应的参数\n",
    "LoRAPromptDataFrame= pd.read_csv('./PromptsForReviews/LoRAPromptsFromCivitai.csv')\n",
    "LoRAPromptDataFrameNotDone = LoRAPromptDataFrame[LoRAPromptDataFrame['Skip']!=True]\n",
    "\n",
    "url = \"http://127.0.0.1:7860\"\n",
    "\n",
    "for i in range(len(LoRAPromptDataFrameNotDone)):\n",
    "    temp = LoRAPromptDataFrameNotDone.iloc[i]\n",
    "    Prompts = temp['Prompts']\n",
    "    sd_model_checkpoint=temp['sd_model_checkpoint']\n",
    "    optionsjson = OptionsSET(sd_model_checkpoint=sd_model_checkpoint,outdir_txt2img_samples = \"./GhostStandardofRatingModel/Image/ImageRatingLoRA\",promptindex='LoRA-'+str(temp['index']))\n",
    "    imagejson = ImageSET(Prompts,cfg_scale=6,batch_size=4,n_iter=5)\n",
    "    optionspost = requests.post(url=f'{url}/sdapi/v1/options', json=optionsjson)\n",
    "    imagepost = requests.post(url=f'{url}/sdapi/v1/txt2img', json=imagejson)\n",
    "    LoRAPromptDataFrame.loc[(LoRAPromptDataFrame['index'] == temp['index'])&(LoRAPromptDataFrame['sd_model_checkpoint'] == temp['sd_model_checkpoint']),'Skip'] = True\n",
    "    LoRAPromptDataFrame.to_csv('./PromptsForReviews/LoRAPromptsFromCivitai.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#GramMatrix for styleloss\n",
    "#通过GramMatrix计算风格损失\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "model=models.vgg19(pretrained=True).features\n",
    "device=torch.device( \"cuda\" if (torch.cuda.is_available()) else 'cpu')\n",
    "\n",
    "def image_loader(path):\n",
    "    image=Image.open(path).convert('RGB')\n",
    "    rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "    rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "    loader=transforms.Compose([transforms.Resize((768,512)), transforms.ToTensor(),transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "    image=loader(image).unsqueeze(0)\n",
    "    return image.to(device,torch.float)\n",
    "\n",
    "#VGG for featuremap\n",
    "#调整VGG输出的featuremap\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG,self).__init__()\n",
    "        self.req_features= ['0','5','10','19','28'] \n",
    "        self.model=models.vgg19(pretrained=True).features[:29] \n",
    "   \n",
    "    def forward(self,x):\n",
    "        features=[]\n",
    "        for layer_num,layer in enumerate(self.model):\n",
    "            x=layer(x)\n",
    "            if (str(layer_num) in self.req_features):\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "model=VGG().to(device).eval()\n",
    "\n",
    "#styleloss of featuremap\n",
    "#计算一个featuremap的styleloss\n",
    "def calc_style_loss(target,style):\n",
    "    batch_size,channel,height,width=target.shape\n",
    "    G=torch.mm(target.view(channel,height*width),target.view(channel,height*width).t())\n",
    "    A=torch.mm(style.view(channel,height*width),style.view(channel,height*width).t())\n",
    "    style_l=torch.mean((G-A)**2)\n",
    "    return style_l\n",
    "\n",
    "#total styleloss\n",
    "#计算一张图的总styleloss\n",
    "def StyleLossImage(targetimagepath,styleimagepath):\n",
    "    targetimage=image_loader(targetimagepath)\n",
    "    styleimage=image_loader(styleimagepath)\n",
    "    target_feature=model(targetimage)\n",
    "    style_feature=model(styleimage)\n",
    "    styleloss= 0\n",
    "    for tar,style in zip(target_feature,style_feature):\n",
    "        styleloss+=calc_style_loss(tar,style)\n",
    "    return(float(styleloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过GramMatrix计算生成图片与目标图片的风格损失\n",
    "#the second part the StyleLossImage\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "path='./Image/ImageRatingLoRA'\n",
    "LoRAFolder = os.listdir(path)\n",
    "\n",
    "LoRADataFrame= pd.read_csv('./PromptsForReviews/LoRAPromptsFromCivitai.csv')\n",
    "try:\n",
    "   DataSummary = pd.read_csv('./ImageRatingSummary/ImageRatingLoRA.csv')\n",
    "   LoRANotDone = [i for i in LoRAFolder if i not in list(DataSummary['name'])]\n",
    "except:\n",
    "   DataSummary = pd.DataFrame()\n",
    "   LoRANotDone = [i for i in LoRAFolder]\n",
    "\n",
    "for i in LoRANotDone:\n",
    "    ImageFolderPATH = os.path.join('./Image/ImageRatingLoRA',i)\n",
    "    temp = list()\n",
    "    name = list()\n",
    "    Imagelist = os.listdir(ImageFolderPATH)\n",
    "    for z in Imagelist:\n",
    "        ImagePath = os.path.join(ImageFolderPATH,z)\n",
    "        TargetImagePATH = os.path.join('./Image/TargetForReviews/LoRAReview',i.split('-')[-1] +'.jpeg')\n",
    "        temp.append(StyleLossImage(ImagePath,TargetImagePATH))\n",
    "        name.append(z)\n",
    "    temp = pd.DataFrame({'StyleLoss':temp})\n",
    "    temp['ImageIndex'] = name\n",
    "    temp.to_csv('./dataresult/ImageRatingDataFrameLoRA/'+i+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataSummary,summary for each prompts\n",
    "#DataSummary,每一个prompt的统计分析\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from PIL import Image\n",
    "\n",
    "path='Image/ImageRatingLoRA'\n",
    "PromptsFolder = os.listdir(\"./\"+path)\n",
    "DataSummary = pd.DataFrame()\n",
    "\n",
    "for i in PromptsFolder:\n",
    "    DataCollect = pd.read_csv(os.path.join('./dataresult/ImageRatingDataFrameLoRA',str(i)+'.csv'))\n",
    "    temp = pd.DataFrame(DataCollect['StyleLoss'].describe()).T\n",
    "    temp.columns = [i+'_LoRALoss' for i in list(temp.columns)]\n",
    "    temp['name'] = str(i)\n",
    "    temp['index'] = str(i).split('-')[-1]\n",
    "    DataSummary = pd.concat([DataSummary,temp],axis=0)\n",
    "DataSummary.to_csv('./ImageRatingSummary/ImageRatingLoRA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.总分及Score\n",
    "### 4.Total Score(Absolute Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "LoRAscore = pd.read_csv('./ImageRatingSummary/ImageRatingLoRA.csv')\n",
    "Imagescore = pd.read_csv('./ImageRatingSummary/ImageRatingSummary.csv')\n",
    "Imagescore['Model'] = Imagescore['name'].apply(lambda x:str(x).split('-')[:-1])\n",
    "Imagescore['Model'] = Imagescore['Model'].apply(lambda x:'-'.join(x))\n",
    "Imagescore = Imagescore[['count_LAIONScore', 'mean_LAIONScore', 'std_LAIONScore',\n",
    "       'min_LAIONScore', '25%_LAIONScore', '50%_LAIONScore', '75%_LAIONScore',\n",
    "       'max_LAIONScore', 'skew_LAIONScore', 'kurtosis_LAIONScore',\n",
    "       'RatingScore_mean', 'RatingScore_std', 'Clipscore_mean',\n",
    "       'Clipscore_std', 'Artifact_mean', 'Artifact_std','Model']]\n",
    "temp = Imagescore.groupby(['Model']).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoRAscore['Model'] = LoRAscore['name'].apply(lambda x:str(x).split('-')[:-2])\n",
    "LoRAscore['Model'] = LoRAscore['Model'].apply(lambda x:'-'.join(x))\n",
    "LoRAscore = LoRAscore[['count_LoRALoss', 'mean_LoRALoss', 'min_LoRALoss','std_LoRALoss','Model']]\n",
    "temp2 = LoRAscore.groupby(['Model']).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BrendanDesktop\\AppData\\Local\\Temp\\ipykernel_8932\\4024998018.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  temp3 = Stylescore.groupby(['Model']).agg('mean')\n"
     ]
    }
   ],
   "source": [
    "Stylescore = pd.read_csv('./ImageRatingSummary/ImageRatingStyle.csv')\n",
    "Stylescore['Model'] = Stylescore['name'].apply(lambda x:str(x).split('-')[:-2])\n",
    "Stylescore['Model'] = Stylescore['Model'].apply(lambda x:'-'.join(x))\n",
    "temp3 = Stylescore.groupby(['Model']).agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(temp,temp2,on='Model')\n",
    "temp = pd.merge(temp,temp3,on='Model',how='outer')\n",
    "temp['count_LAIONScore'] = temp['count_LAIONScore']*30\n",
    "temp['count_LoRALoss'] = temp['count_LoRALoss'] *15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('./ImageRatingSummary/ModelSummary_Total.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.基于每一个Prompts的标准化的分数\n",
    "### 5.Standardize Score of each prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1 image quality\n",
    "#Part1 出图质量\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path='./dataresult/ImageRatingDataFrame'\n",
    "RatingDataFrameFolder = os.listdir(path)\n",
    "Prompt = pd.read_csv('./PromptsForReviews/PromptsSummaryFromCivitai.csv')\n",
    "indexlist = list(set(Prompt['index']))\n",
    "modellist = list(set(Prompt['sd_model_checkpoint']))\n",
    "\n",
    "def standardize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "def normalize(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "alldata = pd.DataFrame()\n",
    "for indexname in indexlist:\n",
    "    indexDataFrame = pd.DataFrame()\n",
    "    for modelname in modellist:\n",
    "        tempname = modelname + '-' + str(indexname)\n",
    "        temp = pd.read_csv(os.path.join(path,tempname) +'.csv')\n",
    "        temp['name'] = tempname\n",
    "        indexDataFrame = pd.concat([indexDataFrame,temp],axis=0)\n",
    "    alldata = pd.concat([alldata,indexDataFrame],axis=0)\n",
    "    indexDataFrame['LAIONScore']=standardize(indexDataFrame['LAIONScore'])\n",
    "    indexDataFrame['ClipScore']=standardize(indexDataFrame['ClipScore'])\n",
    "    indexDataFrame['Rating']=standardize(indexDataFrame['Rating'])\n",
    "    indexDataFrame['Artifact']=standardize(indexDataFrame['Artifact'])\n",
    "    del indexDataFrame['Unnamed: 0']\n",
    "    savename = os.path.join('./dataresultbyPrompt/ImageRatingDataFrame','Prompt-'+str(indexname)+\".csv\")\n",
    "    indexDataFrame.to_csv(savename)\n",
    "#summary\n",
    "path='./dataresultbyPrompt/ImageRatingDataFrame'\n",
    "NormRatingDataFrameFolder = os.listdir(path)\n",
    "AllNormDataFrame = pd.DataFrame()\n",
    "for i in NormRatingDataFrameFolder:\n",
    "    temp = pd.read_csv(os.path.join(path,i))\n",
    "    AllNormDataFrame = pd.concat([AllNormDataFrame,temp],axis=0)\n",
    "AllNormDataFrame['Model'] = AllNormDataFrame['name'].apply(lambda x:str(x).split('-')[:-1])\n",
    "AllNormDataFrame['Model'] = AllNormDataFrame['Model'].apply(lambda x:'-'.join(x))\n",
    "del AllNormDataFrame['name']\n",
    "temp = AllNormDataFrame.groupby('Model').mean().reset_index()\n",
    "del temp['Unnamed: 0']\n",
    "ImagePromptQuality=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Artifact</th>\n",
       "      <th>ClipScore</th>\n",
       "      <th>LAIONScore</th>\n",
       "      <th>ImageIndex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWPainting 1.1_v1.1.safetensors</td>\n",
       "      <td>-0.281444</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.056572</td>\n",
       "      <td>0.626779</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GhostMix-V2.0-fp16-NoVAE</td>\n",
       "      <td>-0.036123</td>\n",
       "      <td>0.198364</td>\n",
       "      <td>0.288943</td>\n",
       "      <td>0.227293</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moyou_v1020.safetensors</td>\n",
       "      <td>-0.636146</td>\n",
       "      <td>0.021177</td>\n",
       "      <td>-1.190967</td>\n",
       "      <td>-0.872010</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dreamshaper_7.safetensors</td>\n",
       "      <td>0.716444</td>\n",
       "      <td>-0.649532</td>\n",
       "      <td>0.583431</td>\n",
       "      <td>0.401422</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lyriel_v14.safetensors</td>\n",
       "      <td>0.517458</td>\n",
       "      <td>-0.615607</td>\n",
       "      <td>0.668897</td>\n",
       "      <td>0.163841</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>majicmixLux_v10.safetensors</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.112501</td>\n",
       "      <td>-0.237406</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>majicmixRealistic_v6.safetensors</td>\n",
       "      <td>-0.482768</td>\n",
       "      <td>0.299762</td>\n",
       "      <td>-0.647942</td>\n",
       "      <td>-0.720982</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>revAnimated_v122.safetensors</td>\n",
       "      <td>0.202579</td>\n",
       "      <td>0.141137</td>\n",
       "      <td>0.353566</td>\n",
       "      <td>0.411064</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model    Rating  Artifact  ClipScore  \\\n",
       "0   AWPainting 1.1_v1.1.safetensors -0.281444  0.604700   0.056572   \n",
       "1          GhostMix-V2.0-fp16-NoVAE -0.036123  0.198364   0.288943   \n",
       "2           Moyou_v1020.safetensors -0.636146  0.021177  -1.190967   \n",
       "3         dreamshaper_7.safetensors  0.716444 -0.649532   0.583431   \n",
       "4            lyriel_v14.safetensors  0.517458 -0.615607   0.668897   \n",
       "5       majicmixLux_v10.safetensors       NaN       NaN  -0.112501   \n",
       "6  majicmixRealistic_v6.safetensors -0.482768  0.299762  -0.647942   \n",
       "7      revAnimated_v122.safetensors  0.202579  0.141137   0.353566   \n",
       "\n",
       "   LAIONScore  ImageIndex  \n",
       "0    0.626779        16.5  \n",
       "1    0.227293        16.5  \n",
       "2   -0.872010        16.5  \n",
       "3    0.401422        16.5  \n",
       "4    0.163841        16.5  \n",
       "5   -0.237406        16.5  \n",
       "6   -0.720982        16.5  \n",
       "7    0.411064        16.5  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImagePromptQuality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alldata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#std of the LAION Score\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#LAION Score std计算\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m alldata[\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m alldata[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39mstr\u001b[39m(x)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m      4\u001b[0m alldata[\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m alldata[\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x:\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(x))\n\u001b[0;32m      5\u001b[0m ImageStd \u001b[39m=\u001b[39m alldata\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mLAIONScore\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstd()\u001b[39m.\u001b[39mreset_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alldata' is not defined"
     ]
    }
   ],
   "source": [
    "#std of the LAION Score\n",
    "#LAION Score std计算\n",
    "alldata['Model'] = alldata['name'].apply(lambda x:str(x).split('-')[:-1])\n",
    "alldata['Model'] = alldata['Model'].apply(lambda x:'-'.join(x))\n",
    "ImageStd = alldata.groupby('Model')['LAIONScore'].std().reset_index()\n",
    "ImageStd = ImageStd.rename(columns={'LAIONScore':'LAIONScore_Std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2.,   1.,   1.,   6.,   9.,  16.,  37.,  41.,  46.,  58.,  61.,\n",
       "         79., 104., 134., 193., 294., 389., 498., 494., 594., 648., 693.,\n",
       "        786., 650., 463., 269., 111.,  32.,   8.,   3.]),\n",
       " array([4.81111813, 4.91020894, 5.00929976, 5.10839057, 5.20748138,\n",
       "        5.3065722 , 5.40566301, 5.50475383, 5.60384464, 5.70293546,\n",
       "        5.80202627, 5.90111709, 6.0002079 , 6.09929872, 6.19838953,\n",
       "        6.29748034, 6.39657116, 6.49566197, 6.59475279, 6.6938436 ,\n",
       "        6.79293442, 6.89202523, 6.99111605, 7.09020686, 7.18929768,\n",
       "        7.28838849, 7.38747931, 7.48657012, 7.58566093, 7.68475175,\n",
       "        7.78384256]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(alldata['LAIONScore'],bins=30, color='skyblue') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BrendanDesktop\\AppData\\Local\\Temp\\ipykernel_3748\\1717847352.py:55: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  temp = AllNormDataFrame.groupby('Model').mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "#Part3 LoRA\n",
    "#Part3 LoRA兼容性\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path='./dataresult/ImageRatingDataFrameLoRA'\n",
    "RatingDataFrameFolder = os.listdir(path)\n",
    "Prompt = pd.read_csv('./PromptsForReviews/LoRAPromptsFromCivitai.csv')\n",
    "indexlist = list(set(Prompt['index']))\n",
    "modellist = list(set(Prompt['sd_model_checkpoint']))\n",
    "\n",
    "def standardize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "def normalize(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "for indexname in indexlist:\n",
    "    indexDataFrame = pd.DataFrame()\n",
    "    for modelname in modellist:\n",
    "        tempname = modelname + '-LoRA-' + str(indexname)\n",
    "        temp = pd.read_csv(os.path.join(path,tempname) +'.csv')\n",
    "        temp['name'] = tempname\n",
    "        indexDataFrame = pd.concat([indexDataFrame,temp],axis=0)\n",
    "    indexDataFrame['StyleLoss']=standardize(indexDataFrame['StyleLoss'])\n",
    "    indexDataFrame = indexDataFrame.rename(columns={'StyleLoss':'LoRALoss'})\n",
    "    del indexDataFrame['Unnamed: 0']\n",
    "    savename = os.path.join('./dataresultbyPrompt/ImageRatingDataFrameLoRA','Prompt-'+str(indexname)+\".csv\")\n",
    "    indexDataFrame.to_csv(savename)\n",
    "#summary\n",
    "path='./dataresultbyPrompt/ImageRatingDataFrameLoRA'\n",
    "NormRatingDataFrameFolder = os.listdir(path)\n",
    "AllNormDataFrame = pd.DataFrame()\n",
    "\n",
    "for i in NormRatingDataFrameFolder:\n",
    "    temp = pd.read_csv(os.path.join(path,i))\n",
    "    AllNormDataFrame = pd.concat([AllNormDataFrame,temp],axis=0)\n",
    "AllNormDataFrame['Model'] = AllNormDataFrame['name'].apply(lambda x:str(x).split('-')[:-2])\n",
    "AllNormDataFrame['Model'] = AllNormDataFrame['Model'].apply(lambda x:'-'.join(x))\n",
    "\n",
    "\n",
    "#drop LoRA image that uses the review model\n",
    "TargetModel = pd.read_csv('./PromptsForReviews/LoRAUseModel.csv')\n",
    "TargetModel['name'] = TargetModel['used_sd_model_checkpoint'] + '-LoRA-' +TargetModel['index'].apply(lambda x :str(x))\n",
    "TargetModel['dropTargetModel'] = True\n",
    "TargetModel = TargetModel[['name','dropTargetModel']]\n",
    "AllWithoutTargetModel = pd.merge(AllNormDataFrame,TargetModel,on='name',how='left')\n",
    "AllWithoutTargetModel = AllWithoutTargetModel[AllWithoutTargetModel['dropTargetModel'] != True]\n",
    "temp2=AllWithoutTargetModel[['Model','LoRALoss']]\n",
    "temp2 = temp2.groupby('Model').mean().reset_index()\n",
    "temp2 = temp2.rename(columns={'LoRALoss':'LoRALoss_NoTM'})\n",
    "LoRAQualityWithoutTM=temp2\n",
    "\n",
    "del AllNormDataFrame['name']\n",
    "temp = AllNormDataFrame.groupby('Model').mean().reset_index()\n",
    "del temp['Unnamed: 0']\n",
    "LoRAQuality=temp\n",
    "LoRAQuality = pd.merge(LoRAQuality,LoRAQualityWithoutTM,on='Model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BrendanDesktop\\AppData\\Local\\Temp\\ipykernel_3748\\3754953642.py:34: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  temp = AllNormDataFrame.groupby('Model').mean().reset_index()\n"
     ]
    }
   ],
   "source": [
    "#Part2 Style\n",
    "#Part2 Style兼容性\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Prompt = pd.read_csv('./PromptsForReviews/StylePrompts.csv')\n",
    "indexlist = list(set(Prompt['index']))\n",
    "modellist = list(set(Prompt['sd_model_checkpoint']))\n",
    "StyleData = pd.read_csv('./ImageRatingSummary/ImageRatingStyle.csv')\n",
    "\n",
    "def standardize(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "def normalize(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))\n",
    "\n",
    "\n",
    "for indexname in indexlist:\n",
    "    temp = StyleData[StyleData['index'] == indexname].copy()\n",
    "    temp['StyleLoss']=standardize(temp['StyleLoss']) \n",
    "    savename = os.path.join('./dataresultbyPrompt/ImageRatingDataFrameStyle','Prompt-'+str(indexname)+\".csv\")\n",
    "    temp.to_csv(savename)\n",
    "\n",
    "#summary\n",
    "path='./dataresultbyPrompt/ImageRatingDataFrameStyle'\n",
    "NormRatingDataFrameFolder = os.listdir(path)\n",
    "AllNormDataFrame = pd.DataFrame()\n",
    "for i in NormRatingDataFrameFolder:\n",
    "    temp = pd.read_csv(os.path.join(path,i))\n",
    "    AllNormDataFrame = pd.concat([AllNormDataFrame,temp],axis=0)\n",
    "AllNormDataFrame['Model'] = AllNormDataFrame['name'].apply(lambda x:str(x).split('-')[:-2])\n",
    "AllNormDataFrame['Model'] = AllNormDataFrame['Model'].apply(lambda x:'-'.join(x))\n",
    "del AllNormDataFrame['name']\n",
    "temp = AllNormDataFrame.groupby('Model').mean().reset_index()\n",
    "del temp['Unnamed: 0'],temp['index']\n",
    "StyleQuality=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(ImagePromptQuality,ImageStd,on='Model')\n",
    "temp = pd.merge(temp,StyleQuality,on='Model')\n",
    "temp = pd.merge(temp,LoRAQuality,on='Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('./ImageRatingSummary/ModelSummary_Standard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Artifact</th>\n",
       "      <th>ClipScore</th>\n",
       "      <th>LAIONScore</th>\n",
       "      <th>ImageIndex</th>\n",
       "      <th>LAIONScore_Std</th>\n",
       "      <th>StyleLoss</th>\n",
       "      <th>LoRALoss</th>\n",
       "      <th>LoRALoss_NoTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AWPainting 1.1_v1.1.safetensors</td>\n",
       "      <td>-0.281444</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.040631</td>\n",
       "      <td>0.583500</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.306680</td>\n",
       "      <td>0.038155</td>\n",
       "      <td>0.540288</td>\n",
       "      <td>0.540288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GhostMix-V2.0-fp16-NoVAE</td>\n",
       "      <td>-0.036123</td>\n",
       "      <td>0.198364</td>\n",
       "      <td>0.262241</td>\n",
       "      <td>0.191307</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.388465</td>\n",
       "      <td>-0.419657</td>\n",
       "      <td>-0.114531</td>\n",
       "      <td>-0.114531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moyou_v1020.safetensors</td>\n",
       "      <td>-0.636146</td>\n",
       "      <td>0.021177</td>\n",
       "      <td>-1.164639</td>\n",
       "      <td>-0.893277</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.374577</td>\n",
       "      <td>0.575342</td>\n",
       "      <td>0.280419</td>\n",
       "      <td>0.280419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dreamshaper_7.safetensors</td>\n",
       "      <td>0.716444</td>\n",
       "      <td>-0.649532</td>\n",
       "      <td>0.546587</td>\n",
       "      <td>0.361628</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.388404</td>\n",
       "      <td>-0.020113</td>\n",
       "      <td>-0.110264</td>\n",
       "      <td>-0.110264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lyriel_v14.safetensors</td>\n",
       "      <td>0.517458</td>\n",
       "      <td>-0.615607</td>\n",
       "      <td>0.631526</td>\n",
       "      <td>0.132839</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.433813</td>\n",
       "      <td>0.085033</td>\n",
       "      <td>-0.080010</td>\n",
       "      <td>-0.080010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>majicmixRealistic_v6.safetensors</td>\n",
       "      <td>-0.482768</td>\n",
       "      <td>0.299762</td>\n",
       "      <td>-0.642076</td>\n",
       "      <td>-0.745622</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.355439</td>\n",
       "      <td>0.201756</td>\n",
       "      <td>0.023498</td>\n",
       "      <td>0.074825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>revAnimated_v122.safetensors</td>\n",
       "      <td>0.202579</td>\n",
       "      <td>0.141137</td>\n",
       "      <td>0.325730</td>\n",
       "      <td>0.369625</td>\n",
       "      <td>16.5</td>\n",
       "      <td>0.386477</td>\n",
       "      <td>-0.460516</td>\n",
       "      <td>-0.539401</td>\n",
       "      <td>-0.476623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Model    Rating  Artifact  ClipScore  \\\n",
       "0   AWPainting 1.1_v1.1.safetensors -0.281444  0.604700   0.040631   \n",
       "1          GhostMix-V2.0-fp16-NoVAE -0.036123  0.198364   0.262241   \n",
       "2           Moyou_v1020.safetensors -0.636146  0.021177  -1.164639   \n",
       "3         dreamshaper_7.safetensors  0.716444 -0.649532   0.546587   \n",
       "4            lyriel_v14.safetensors  0.517458 -0.615607   0.631526   \n",
       "5  majicmixRealistic_v6.safetensors -0.482768  0.299762  -0.642076   \n",
       "6      revAnimated_v122.safetensors  0.202579  0.141137   0.325730   \n",
       "\n",
       "   LAIONScore  ImageIndex  LAIONScore_Std  StyleLoss  LoRALoss  LoRALoss_NoTM  \n",
       "0    0.583500        16.5        0.306680   0.038155  0.540288       0.540288  \n",
       "1    0.191307        16.5        0.388465  -0.419657 -0.114531      -0.114531  \n",
       "2   -0.893277        16.5        0.374577   0.575342  0.280419       0.280419  \n",
       "3    0.361628        16.5        0.388404  -0.020113 -0.110264      -0.110264  \n",
       "4    0.132839        16.5        0.433813   0.085033 -0.080010      -0.080010  \n",
       "5   -0.745622        16.5        0.355439   0.201756  0.023498       0.074825  \n",
       "6    0.369625        16.5        0.386477  -0.460516 -0.539401      -0.476623  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
